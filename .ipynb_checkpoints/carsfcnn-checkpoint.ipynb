{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d7e011-23ca-4040-a3dc-583f3ee781dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# Define the path to your data folders\n",
    "train_data_path = '/Users/ummefahmidaakter/Downloads/cars/Train'\n",
    "test_data_path = '/Users/ummefahmidaakter/Downloads/cars/Test'\n",
    "\n",
    "# Define transforms to preprocess the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize the images to 224x224 pixels\n",
    "    transforms.ToTensor(),  # convert the images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize the images\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "# Define a DataLoader to load the data in batches during training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dfe3ede-29de-4080-b95a-c599f07826e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iter [1/4], Loss: 1.1020\n",
      "Epoch [2/10], Iter [1/4], Loss: 2.4884\n",
      "Epoch [3/10], Iter [1/4], Loss: 1.7794\n",
      "Epoch [4/10], Iter [1/4], Loss: 2.1000\n",
      "Epoch [5/10], Iter [1/4], Loss: 1.5948\n",
      "Epoch [6/10], Iter [1/4], Loss: 0.4654\n",
      "Epoch [7/10], Iter [1/4], Loss: 1.5268\n",
      "Epoch [8/10], Iter [1/4], Loss: 0.2683\n",
      "Epoch [9/10], Iter [1/4], Loss: 0.8797\n",
      "Epoch [10/10], Iter [1/4], Loss: 0.9538\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=256, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(224 * 224 * 3, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224 * 224 * 3)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network model\n",
    "net = Net()\n",
    "\n",
    "# Define the optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d], Loss: %.4f' % (epoch+1, 10, i+1, len(train_dataset)//16, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ffbe-1095-4f78-9e44-cfe62b100319",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "The input size of the model is 224 x 224 x 3 = 150,528, which is flattened to a 1D tensor of size 1 x 150,528. \n",
    "The model also includes dropout with a probability of 0.2, which helps in reducing overfitting. \n",
    "The number of epochs and learning rate used for training are 10 and 0.001, respectively. \n",
    "The batch size used for training is 16.\n",
    "A grid search is performed to find the best hyperparameters for the model, including the number of neurons in the first hidden layer, \n",
    "dropout probability, learning rate, and batch size.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a09f50-df13-4762-b71a-14ba18e517a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: Hidden size: 256, Dropout: 0.2, Learning rate: 0.010, Batch size: 32, Accuracy: 67.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "\n",
    "hidden_sizes = [128, 256, 512]\n",
    "dropouts = [0.2, 0.4, 0.6]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "# Initialize variables to store the best accuracy and corresponding hyperparameters\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Loop through all possible combinations of hyperparameters\n",
    "for params in itertools.product(hidden_sizes, dropouts, learning_rates, batch_sizes):\n",
    "    \n",
    "    # Unpack the hyperparameters\n",
    "    hidden_size, dropout, lr, batch_size = params\n",
    "    # Create a neural network with the given hyperparameters\n",
    "    net = Net(hidden_size=hidden_size, dropout=dropout)\n",
    "    # Define an optimizer with the given learning rate\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    # Create data loaders for the training and test sets with the given batch size\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Train the network for 10 epochs on the training set\n",
    "    for epoch in range(10):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Evaluate the network on the test set and calculate its accuracy\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            if total > 0:\n",
    "                accuracy = 100 * correct / total\n",
    "            else:\n",
    "                accuracy = 0\n",
    "        # Update the best accuracy and corresponding hyperparameters if the current accuracy is better\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'hidden_size': hidden_size, 'dropout': dropout, 'lr': lr, 'batch_size': batch_size}\n",
    "        \n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print('Best hyperparameters: Hidden size: %d, Dropout: %.1f, Learning rate: %.3f, Batch size: %d, Accuracy: %.2f' % (best_params['hidden_size'], best_params['dropout'], best_params['lr'], best_params['batch_size'], best_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b74bff-443d-4fc5-9d1f-9d9a1ac02fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Confusion Matrix=[[1 0 2]\n",
      " [0 1 1]\n",
      " [0 1 1]], Accuracy=0.4286, Precision=0.6429, Recall=0.4286, F1 Score=0.4524, Specificity=1.0000, Sensitivity=1.0000, Kappa=0.1765\n",
      "Fold 2: Confusion Matrix=[[2 0 0]\n",
      " [0 0 0]\n",
      " [2 1 2]], Accuracy=0.5714, Precision=0.8571, Recall=0.5714, F1 Score=0.5986, Specificity=1.0000, Sensitivity=0.0000, Kappa=0.3226\n",
      "Fold 3: Confusion Matrix=[[0 1 0]\n",
      " [1 0 2]\n",
      " [2 0 1]], Accuracy=0.1429, Precision=0.1429, Recall=0.1429, F1 Score=0.1429, Specificity=0.0000, Sensitivity=0.0000, Kappa=-0.2353\n",
      "Fold 4: Confusion Matrix=[[2 1 0]\n",
      " [0 2 0]\n",
      " [0 2 0]], Accuracy=0.5714, Precision=0.8286, Recall=0.5714, F1 Score=0.5061, Specificity=0.6667, Sensitivity=1.0000, Kappa=0.3636\n",
      "Fold 5: Confusion Matrix=[[0 1 2]\n",
      " [0 1 1]\n",
      " [0 1 0]], Accuracy=0.1667, Precision=0.6111, Recall=0.1667, F1 Score=0.1333, Specificity=0.0000, Sensitivity=1.0000, Kappa=-0.1111\n",
      "Fold 6: Confusion Matrix=[[2 0 2]\n",
      " [0 0 1]\n",
      " [0 0 1]], Accuracy=0.5000, Precision=0.8750, Recall=0.5000, F1 Score=0.5111, Specificity=1.0000, Sensitivity=0.0000, Kappa=0.2500\n",
      "Fold 7: Confusion Matrix=[[0 1 1]\n",
      " [0 1 2]\n",
      " [0 0 1]], Accuracy=0.3333, Precision=0.6250, Recall=0.3333, F1 Score=0.2667, Specificity=0.0000, Sensitivity=1.0000, Kappa=0.0769\n",
      "Fold 8: Confusion Matrix=[[0 0 1]\n",
      " [1 2 0]\n",
      " [0 0 2]], Accuracy=0.6667, Precision=0.7222, Recall=0.6667, F1 Score=0.6667, Specificity=0.0000, Sensitivity=0.6667, Kappa=0.4783\n",
      "Fold 9: Confusion Matrix=[[1 0 0]\n",
      " [1 1 0]\n",
      " [1 2 0]], Accuracy=0.3333, Precision=0.6667, Recall=0.3333, F1 Score=0.2167, Specificity=1.0000, Sensitivity=0.5000, Kappa=0.1111\n",
      "Fold 10: Confusion Matrix=[[0 0 0]\n",
      " [0 0 1]\n",
      " [1 2 2]], Accuracy=0.3333, Precision=0.5556, Recall=0.3333, F1 Score=0.4167, Specificity=0.0000, Sensitivity=0.0000, Kappa=-0.2632\n",
      "Average Performance Metrics: Accuracy=0.4048, Precision=0.6527, Recall=0.4048, F1 Score=0.3911, Specificity=0.4667, Sensitivity=0.5167, Kappa=-0.2632\n",
      "Average Confusion Matrix:\n",
      " [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n",
      "Best fold: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=256, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(224 * 224 * 3, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224 * 224 * 3)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the k-fold cross-validation\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "best_metric = 0.0\n",
    "best_fold = 0\n",
    "\n",
    "# Train and evaluate the model on each fold\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "specificity_list = []\n",
    "sensitivity_list = []\n",
    "confusion_matrix_list = []\n",
    "kappa_list = []\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(train_dataset)):\n",
    "    train_dataset_fold = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    test_dataset_fold = torch.utils.data.Subset(train_dataset, test_indices)\n",
    "    train_loader_fold = torch.utils.data.DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n",
    "    test_loader_fold = torch.utils.data.DataLoader(test_dataset_fold, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    net = Net(hidden_size=256, dropout=0.2)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.010)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(10):\n",
    "        for i, (images, labels) in enumerate(train_loader_fold):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_fold:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_images += labels.size(0)\n",
    "            true_labels.extend(labels.tolist())\n",
    "            predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "    # Calculate the performance metrics\n",
    "    accuracy = total_correct / total_images\n",
    "    precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "    confusion_matrix = metrics.confusion_matrix(true_labels, predicted_labels, labels=[0, 1, 2])\n",
    "    tp = confusion_matrix[1][1]\n",
    "    tn = confusion_matrix[0][0]\n",
    "    fp = confusion_matrix[0][1]\n",
    "    fn = confusion_matrix[1][0]\n",
    "    specificity = tn / (tn + fp + 1e-10)\n",
    "    sensitivity = tp / (tp + fn + 1e-10)\n",
    "    kappa = metrics.cohen_kappa_score(true_labels, predicted_labels)\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "    specificity_list.append(specificity)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    confusion_matrix_list.append(confusion_matrix)\n",
    "    kappa_list.append(kappa)\n",
    "    print('Fold %d: Confusion Matrix=%s, Accuracy=%.4f, Precision=%.4f, Recall=%.4f, F1 Score=%.4f, Specificity=%.4f, Sensitivity=%.4f, Kappa=%.4f' % (fold+1, confusion_matrix, accuracy, precision, recall, f1_score, specificity, sensitivity, kappa))\n",
    "    # Update best fold if current fold has a better metric\n",
    "    if accuracy > best_metric:\n",
    "        best_metric = accuracy\n",
    "        best_fold = fold\n",
    "    \n",
    "# Calculate the average performance metrics\n",
    "avg_accuracy = np.mean(accuracy_list)\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_f1_score = np.mean(f1_score_list)\n",
    "avg_specificity = np.mean(specificity_list)\n",
    "avg_sensitivity = np.mean(sensitivity_list)\n",
    "avg_confusion_matrix = np.zeros((3, 3), dtype=int)\n",
    "for cm in confusion_matrix_list:\n",
    "    # Get the shape of the confusion matrix and use it to dynamically set the shape of the average confusion matrix\n",
    "    cm_shape = cm.shape\n",
    "    avg_confusion_matrix += np.pad(cm, [(0, 3 - cm_shape[0]), (0, 3 - cm_shape[1])], mode='constant', constant_values=0)\n",
    "avg_confusion_matrix = avg_confusion_matrix // k\n",
    "tn = avg_confusion_matrix[0, 0]\n",
    "fp = avg_confusion_matrix[0, 1]\n",
    "fn = avg_confusion_matrix[1, 0]\n",
    "tp = avg_confusion_matrix[1, 1]\n",
    "avg_kappa = metrics.cohen_kappa_score(true_labels, predicted_labels)\n",
    "\n",
    "print('Average Performance Metrics: Accuracy=%.4f, Precision=%.4f, Recall=%.4f, F1 Score=%.4f, Specificity=%.4f, Sensitivity=%.4f, Kappa=%.4f' % (avg_accuracy, avg_precision, avg_recall, avg_f1_score, avg_specificity, avg_sensitivity, avg_kappa))\n",
    "print('Average Confusion Matrix:\\n', avg_confusion_matrix)\n",
    "print('Best fold:', best_fold+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a876c-b5d2-4807-91ef-10da600f5760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2acd58-8ae2-4b4c-9511-8ecaa0bacb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
